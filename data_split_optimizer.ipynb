{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gray value 2:\n",
      "  Number of images: 2824\n",
      "  Average proportion: 0.1361\n",
      "  Min proportion: 0.0000\n",
      "  Max proportion: 0.7083\n",
      "\n",
      "Gray value 7:\n",
      "  Number of images: 3047\n",
      "  Average proportion: 0.1368\n",
      "  Min proportion: 0.0013\n",
      "  Max proportion: 0.5324\n",
      "\n",
      "Gray value 8:\n",
      "  Number of images: 3056\n",
      "  Average proportion: 0.5345\n",
      "  Min proportion: 0.0893\n",
      "  Max proportion: 0.9534\n",
      "\n",
      "Gray value 9:\n",
      "  Number of images: 961\n",
      "  Average proportion: 0.0770\n",
      "  Min proportion: 0.0000\n",
      "  Max proportion: 0.5249\n",
      "\n",
      "Gray value 15:\n",
      "  Number of images: 1757\n",
      "  Average proportion: 0.0114\n",
      "  Min proportion: 0.0000\n",
      "  Max proportion: 0.1909\n",
      "\n",
      "Gray value 16:\n",
      "  Number of images: 855\n",
      "  Average proportion: 0.0042\n",
      "  Min proportion: 0.0000\n",
      "  Max proportion: 0.1843\n",
      "\n",
      "Gray value 18:\n",
      "  Number of images: 2971\n",
      "  Average proportion: 0.1672\n",
      "  Min proportion: 0.0000\n",
      "  Max proportion: 0.6761\n",
      "\n",
      "Gray value 11:\n",
      "  Number of images: 24\n",
      "  Average proportion: 0.0040\n",
      "  Min proportion: 0.0000\n",
      "  Max proportion: 0.0206\n",
      "\n",
      "Gray value 14:\n",
      "  Number of images: 241\n",
      "  Average proportion: 0.0257\n",
      "  Min proportion: 0.0000\n",
      "  Max proportion: 0.2406\n",
      "\n",
      "Gray value 12:\n",
      "  Number of images: 285\n",
      "  Average proportion: 0.0053\n",
      "  Min proportion: 0.0000\n",
      "  Max proportion: 0.1087\n",
      "\n",
      "Gray value 5:\n",
      "  Number of images: 96\n",
      "  Average proportion: 0.0335\n",
      "  Min proportion: 0.0000\n",
      "  Max proportion: 0.1630\n",
      "\n",
      "Gray value 6:\n",
      "  Number of images: 3\n",
      "  Average proportion: 0.0000\n",
      "  Min proportion: 0.0000\n",
      "  Max proportion: 0.0000\n",
      "\n",
      "Gray value 4:\n",
      "  Number of images: 223\n",
      "  Average proportion: 0.0636\n",
      "  Min proportion: 0.0000\n",
      "  Max proportion: 0.3387\n",
      "\n",
      "Gray value 3:\n",
      "  Number of images: 92\n",
      "  Average proportion: 0.0104\n",
      "  Min proportion: 0.0000\n",
      "  Max proportion: 0.0581\n",
      "\n",
      "Gray value 10:\n",
      "  Number of images: 44\n",
      "  Average proportion: 0.0120\n",
      "  Min proportion: 0.0004\n",
      "  Max proportion: 0.0430\n",
      "\n",
      "Gray value 13:\n",
      "  Number of images: 6\n",
      "  Average proportion: 0.0020\n",
      "  Min proportion: 0.0003\n",
      "  Max proportion: 0.0034\n",
      "\n",
      "Visualization saved to visualization_results/dataset_review/grayscale_distribution.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def collect_grayscale_distributions(root_dirs):\n",
    "    distributions = defaultdict(list)\n",
    "    \n",
    "    for root_dir in root_dirs:\n",
    "        index_label_dir = os.path.join(root_dir, 'indexLabel')\n",
    "        \n",
    "        if not os.path.exists(index_label_dir):\n",
    "            print(f\"Warning: {index_label_dir} does not exist.\")\n",
    "            continue\n",
    "        \n",
    "        for filename in os.listdir(index_label_dir):\n",
    "            if filename.endswith('.png'):\n",
    "                file_path = os.path.join(index_label_dir, filename)\n",
    "                try:\n",
    "                    img = Image.open(file_path).convert('L')\n",
    "                    img_np = np.array(img)\n",
    "                    \n",
    "                    # 排除 0, 1, 17 这些灰度值\n",
    "                    mask = np.isin(img_np, [0, 1, 17], invert=True)\n",
    "                    valid_pixels = img_np[mask]\n",
    "                    \n",
    "                    if valid_pixels.size > 0:\n",
    "                        unique, counts = np.unique(valid_pixels, return_counts=True)\n",
    "                        total_valid_pixels = valid_pixels.size\n",
    "                        \n",
    "                        # 为每个灰度值计算比例\n",
    "                        for gray_value, count in zip(unique, counts):\n",
    "                            distributions[gray_value].append(count / total_valid_pixels)\n",
    "                    else:\n",
    "                        print(f\"Warning: No valid pixels in {file_path}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    return distributions\n",
    "\n",
    "def visualize_grayscale_distribution(distributions, save_path):\n",
    "    # 计算每个灰度值的平均比例\n",
    "    avg_proportions = {k: np.mean(v) for k, v in distributions.items()}\n",
    "    \n",
    "    # 排序以便更好地展示\n",
    "    sorted_items = sorted(avg_proportions.items(), key=lambda x: x[1], reverse=True)\n",
    "    gray_values, proportions = zip(*sorted_items)\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.barplot(x=list(gray_values), y=list(proportions))\n",
    "    plt.title('Average Grayscale Value Distribution')\n",
    "    plt.xlabel('Grayscale Value')\n",
    "    plt.ylabel('Average Proportion')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dirs = [\n",
    "        os.path.join('..', 'data', 'WildScenes', 'WildScenes2d', 'V-01'),\n",
    "        os.path.join('..', 'data', 'WildScenes', 'WildScenes2d', 'V-02'),\n",
    "        os.path.join('..', 'data', 'WildScenes', 'WildScenes2d', 'V-03')\n",
    "    ]\n",
    "\n",
    "    grayscale_distributions = collect_grayscale_distributions(root_dirs)\n",
    "\n",
    "    # 打印每个灰度值的统计信息\n",
    "    for gray_value, proportions in grayscale_distributions.items():\n",
    "        print(f\"Gray value {gray_value}:\")\n",
    "        print(f\"  Number of images: {len(proportions)}\")\n",
    "        print(f\"  Average proportion: {np.mean(proportions):.4f}\")\n",
    "        print(f\"  Min proportion: {np.min(proportions):.4f}\")\n",
    "        print(f\"  Max proportion: {np.max(proportions):.4f}\")\n",
    "        print()\n",
    "\n",
    "    # 可视化灰度值分布\n",
    "    save_dir=os.path.join(\"visualization_results\",\"dataset_review\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, 'grayscale_distribution.png')\n",
    "    visualize_grayscale_distribution(grayscale_distributions, save_path)\n",
    "    print(f\"Visualization saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def collect_grayscale_distributions(root_dirs):\n",
    "    distributions = defaultdict(list)\n",
    "    image_distributions = {}\n",
    "    \n",
    "    for root_dir in root_dirs:\n",
    "        index_label_dir = os.path.join(root_dir, 'indexLabel')\n",
    "        image_dir = os.path.join(root_dir, 'image')\n",
    "        \n",
    "        if not os.path.exists(index_label_dir) or not os.path.exists(image_dir):\n",
    "            continue\n",
    "        \n",
    "        for filename in os.listdir(index_label_dir):\n",
    "            if filename.endswith('.png'):\n",
    "                label_path = os.path.join(index_label_dir, filename)\n",
    "                image_path = os.path.join(image_dir, filename)\n",
    "                \n",
    "                if not os.path.exists(image_path):\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    img = Image.open(label_path).convert('L')\n",
    "                    img_np = np.array(img)\n",
    "                    \n",
    "                    mask = np.isin(img_np, [0, 1, 17], invert=True)\n",
    "                    valid_pixels = img_np[mask]\n",
    "                    \n",
    "                    if valid_pixels.size > 0:\n",
    "                        unique, counts = np.unique(valid_pixels, return_counts=True)\n",
    "                        total_valid_pixels = valid_pixels.size\n",
    "                        \n",
    "                        img_distribution = {}\n",
    "                        for gray_value, count in zip(unique, counts):\n",
    "                            proportion = count / total_valid_pixels\n",
    "                            distributions[gray_value].append(proportion)\n",
    "                            img_distribution[gray_value] = proportion\n",
    "                        \n",
    "                        image_distributions[image_path] = (img_distribution, label_path)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    \n",
    "    return distributions, image_distributions\n",
    "\n",
    "def get_main_classes(image_distributions, min_threshold=0.05, max_threshold=0.2):\n",
    "    main_classes = {}\n",
    "    class_counts = defaultdict(int)\n",
    "    total_images = len(image_distributions)\n",
    "    \n",
    "    for _, (dist, _) in image_distributions.items():\n",
    "        for cls in dist.keys():\n",
    "            class_counts[cls] += 1\n",
    "    \n",
    "    rare_classes = {cls for cls, count in class_counts.items() if count < total_images * 0.05}\n",
    "    \n",
    "    for image_path, (dist, _) in image_distributions.items():\n",
    "        significant_classes = [cls for cls, prop in dist.items() if (cls in rare_classes and prop >= min_threshold) or prop >= max_threshold]\n",
    "        if not significant_classes:\n",
    "            significant_classes = [max(dist, key=dist.get)]\n",
    "        main_classes[image_path] = tuple(significant_classes)\n",
    "    \n",
    "    return main_classes\n",
    "\n",
    "def augment_image(image_path, label_path):\n",
    "    image = Image.open(image_path)\n",
    "    label = Image.open(label_path)\n",
    "    augmented_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    augmented_label = label.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    enhancer = ImageEnhance.Brightness(augmented_image)\n",
    "    augmented_image = enhancer.enhance(1.2)\n",
    "    return augmented_image, augmented_label\n",
    "\n",
    "def augment_rare_classes(image_distributions, main_classes, threshold=10):\n",
    "    augmented_data = {}\n",
    "    class_counts = defaultdict(int)\n",
    "    \n",
    "    for img, classes in main_classes.items():\n",
    "        for cls in classes:\n",
    "            class_counts[cls] += 1\n",
    "    \n",
    "    for cls, count in class_counts.items():\n",
    "        if count < threshold:\n",
    "            for img in [img for img, classes in main_classes.items() if cls in classes]:\n",
    "                aug_image, aug_label = augment_image(img, image_distributions[img][1])\n",
    "                aug_image_path = img.replace('.png', '_aug.png')\n",
    "                aug_label_path = image_distributions[img][1].replace('.png', '_aug.png')\n",
    "                aug_image.save(aug_image_path)\n",
    "                aug_label.save(aug_label_path)\n",
    "                augmented_data[aug_image_path] = (image_distributions[img][0], aug_label_path)\n",
    "    \n",
    "    image_distributions.update(augmented_data)\n",
    "    return image_distributions\n",
    "\n",
    "def stratified_split(image_distributions, train_ratio=0.7, valid_ratio=0.2):\n",
    "    main_classes = get_main_classes(image_distributions)\n",
    "    all_classes = set(cls for classes in main_classes.values() for cls in classes)\n",
    "    \n",
    "    train_set, valid_set, test_set = [], [], []\n",
    "    class_to_images = defaultdict(list)\n",
    "    \n",
    "    for img, classes in main_classes.items():\n",
    "        for cls in classes:\n",
    "            class_to_images[cls].append(img)\n",
    "    \n",
    "    for cls in all_classes:\n",
    "        cls_images = class_to_images[cls]\n",
    "        random.shuffle(cls_images)\n",
    "        \n",
    "        n_train = max(int(len(cls_images) * train_ratio), 1)\n",
    "        n_valid = max(int(len(cls_images) * valid_ratio), 1)\n",
    "        \n",
    "        train_set.extend((img, image_distributions[img][1]) for img in cls_images[:n_train])\n",
    "        valid_set.extend((img, image_distributions[img][1]) for img in cls_images[n_train:n_train+n_valid])\n",
    "        test_set.extend((img, image_distributions[img][1]) for img in cls_images[n_train+n_valid:])\n",
    "    \n",
    "    return list(set(train_set)), list(set(valid_set)), list(set(test_set))\n",
    "\n",
    "def save_split_results(train_set, valid_set, test_set, csv_files):\n",
    "    os.makedirs(os.path.dirname(csv_files['train']), exist_ok=True)\n",
    "    \n",
    "    pd.DataFrame(train_set, columns=['image', 'label']).to_csv(csv_files['train'], index=False)\n",
    "    pd.DataFrame(valid_set, columns=['image', 'label']).to_csv(csv_files['valid'], index=False)\n",
    "    pd.DataFrame(test_set, columns=['image', 'label']).to_csv(csv_files['test'], index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dirs = [\n",
    "        os.path.join('..', 'data', 'WildScenes', 'WildScenes2d', 'V-01'),\n",
    "        os.path.join('..', 'data', 'WildScenes', 'WildScenes2d', 'V-02'),\n",
    "        os.path.join('..', 'data', 'WildScenes', 'WildScenes2d', 'V-03')\n",
    "    ]\n",
    "\n",
    "    _, image_distributions = collect_grayscale_distributions(root_dirs)\n",
    "    image_distributions = augment_rare_classes(image_distributions, get_main_classes(image_distributions))\n",
    "    train_set, valid_set, test_set = stratified_split(image_distributions)\n",
    "\n",
    "    _data_list_dir = os.path.join('datasets', 'data_list')\n",
    "    _csv_files = {\n",
    "        'train': os.path.join(_data_list_dir, 'train.csv'),\n",
    "        'valid': os.path.join(_data_list_dir, 'valid.csv'),\n",
    "        'test': os.path.join(_data_list_dir, 'test.csv'),\n",
    "    }\n",
    "\n",
    "    save_split_results(train_set, valid_set, test_set, _csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cached results from class_image_cache.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 2594 images\n",
      "Class 5: 2790 images\n",
      "Class 6: 2799 images\n",
      "Class 7: 846 images\n",
      "Class 12: 1598 images\n",
      "Class 13: 818 images\n",
      "Class 16: 2675 images\n",
      "Class 14: 2717 images\n",
      "Class 11: 232 images\n",
      "Class 9: 283 images\n",
      "Class 3: 96 images\n",
      "Class 2: 194 images\n",
      "Class 1: 67 images\n",
      "Class 8: 44 images\n",
      "Class 4: 2 images\n",
      "Class 10: 6 images\n",
      "Loaded cached results from class_image_cache.json\n",
      "\n",
      "Class 4 images:\n",
      "../data/WildScenes/WildScenes2d/V-02/image/1623370474-625888651.png\n",
      "../data/WildScenes/WildScenes2d/V-02/image/1623370640-449779203.png\n",
      "\n",
      "Class 14 images:\n",
      "../data/WildScenes/WildScenes2d/V-01/image/1623377790-818434554.png\n",
      "../data/WildScenes/WildScenes2d/V-01/image/1623377792-158486120.png\n",
      "../data/WildScenes/WildScenes2d/V-01/image/1623377796-111739618.png\n",
      "../data/WildScenes/WildScenes2d/V-01/image/1623377799-999188899.png\n",
      "../data/WildScenes/WildScenes2d/V-01/image/1623377800-668195159.png\n",
      "../data/WildScenes/WildScenes2d/V-01/image/1623377804-620552356.png\n",
      "../data/WildScenes/WildScenes2d/V-01/image/1623377808-573506127.png\n",
      "../data/WildScenes/WildScenes2d/V-01/image/1623377812-526645347.png\n",
      "../data/WildScenes/WildScenes2d/V-01/image/1623377816-412682847.png\n",
      "../data/WildScenes/WildScenes2d/V-01/image/1623377820-365669803.png\n",
      "... and 2707 more\n",
      "\n",
      "汇总结果：\n",
      "  Class 0: 2594 images\n",
      "  Class 5: 2790 images\n",
      "  Class 6: 2799 images\n",
      "  Class 7: 846 images\n",
      "  Class 12: 1598 images\n",
      "  Class 13: 818 images\n",
      "  Class 16: 2675 images\n",
      "  Class 14: 2717 images\n",
      "  Class 11: 232 images\n",
      "  Class 9: 283 images\n",
      "  Class 3: 96 images\n",
      "  Class 2: 194 images\n",
      "  Class 1: 67 images\n",
      "  Class 8: 44 images\n",
      "  Class 4: 2 images\n",
      "  Class 10: 6 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def find_images_with_classes(root_dirs, label_to_trainid, target_classes=None, cache_file='class_image_cache.json'):\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'r') as f:\n",
    "            class_to_images = json.load(f)\n",
    "        print(f\"Loaded cached results from {cache_file}\")\n",
    "    else:\n",
    "        class_to_images = defaultdict(list)\n",
    "        for root_dir in root_dirs:\n",
    "            index_label_dir = os.path.join(root_dir, 'indexLabel')\n",
    "            image_dir = os.path.join(root_dir, 'image')\n",
    "            if not os.path.exists(index_label_dir) or not os.path.exists(image_dir):\n",
    "                print(f\"Warning: {index_label_dir} or {image_dir} does not exist.\")\n",
    "                continue\n",
    "            for filename in tqdm(os.listdir(index_label_dir), desc=f\"Scanning images in {root_dir}\"):\n",
    "                if filename.endswith('.png'):\n",
    "                    label_path = os.path.join(index_label_dir, filename)\n",
    "                    image_path = os.path.join(image_dir, filename)\n",
    "                    if not os.path.exists(image_path):\n",
    "                        print(f\"Warning: Image file not found for {filename}\")\n",
    "                        continue\n",
    "                    label = np.array(Image.open(label_path))\n",
    "                    unique_classes = np.unique(label)\n",
    "                    for cls in unique_classes:\n",
    "                        if cls in label_to_trainid:\n",
    "                            train_id = label_to_trainid[cls]\n",
    "                            class_to_images[str(train_id)].append(image_path)\n",
    "        with open(cache_file, 'w') as f:\n",
    "            json.dump(class_to_images, f)\n",
    "        print(f\"Saved results to {cache_file}\")\n",
    "    \n",
    "    if target_classes:\n",
    "        return {cls: class_to_images[str(cls)] for cls in target_classes if str(cls) in class_to_images}\n",
    "    return {int(k): v for k, v in class_to_images.items()}\n",
    "\n",
    "def visualize_class_distribution(class_images, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    classes = list(class_images.keys())\n",
    "    image_counts = [len(images) for images in class_images.values()]\n",
    "\n",
    "    # 条形图\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.barplot(x=classes, y=image_counts)\n",
    "    plt.title('Number of Images per Class')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of Images')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'class_distribution_bar.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # 饼图\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.pie(image_counts, labels=classes, autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('Class Distribution (Pie Chart)')\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'class_distribution_pie.png'))\n",
    "    plt.close()\n",
    "\n",
    "def visualize_class_distribution_heatmap(class_images, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    classes = list(class_images.keys())\n",
    "    image_counts = [len(images) for images in class_images.values()]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(np.array(image_counts).reshape(-1, 1), annot=True, fmt='d', cmap='YlOrRd', xticklabels=['Number of Images'], yticklabels=classes)\n",
    "    plt.title('Class Distribution Heatmap')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Class')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'class_distribution_heatmap.png'))\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_dir=os.path.join(\"visualization_results\",\"dataset_review\")\n",
    "    root_dirs = [\n",
    "        os.path.join('..', 'data', 'WildScenes', 'WildScenes2d', 'V-01'),\n",
    "        os.path.join('..', 'data', 'WildScenes', 'WildScenes2d', 'V-02'),\n",
    "        os.path.join('..', 'data', 'WildScenes', 'WildScenes2d', 'V-03')\n",
    "    ]\n",
    "    \n",
    "    label_to_trainid = {\n",
    "        0: 15, 1: 16, 2: 0, 3: 1, 4: 2, 5: 3, 6: 4, 7: 5, 8: 6, 9: 7,\n",
    "        10: 8, 11: 16, 12: 9, 13: 10, 14: 11, 15: 12, 16: 13, 17: 16, 18: 14\n",
    "    }\n",
    "    \n",
    "    # 找出所有类别的图片\n",
    "    all_class_images = find_images_with_classes(root_dirs, label_to_trainid)\n",
    "    \n",
    "    # 可视化整体类别分布\n",
    "    visualize_class_distribution(all_class_images, save_dir)\n",
    "    \n",
    "    # 可视化热力图\n",
    "    visualize_class_distribution_heatmap(all_class_images, save_dir)\n",
    "\n",
    "    # 打印每个类别的图片数量\n",
    "    for cls, images in all_class_images.items():\n",
    "        print(f\"Class {cls}: {len(images)} images\")\n",
    "    \n",
    "    # 找出特定类别的图片（例如，样本较少的类别）\n",
    "    rare_classes = [4, 14]  # 替换为您感兴趣的类别\n",
    "    rare_class_images = find_images_with_classes(root_dirs, label_to_trainid, target_classes=rare_classes)\n",
    "    \n",
    "    # 打印包含稀有类别的图片路径\n",
    "    for cls, images in rare_class_images.items():\n",
    "        print(f\"\\nClass {cls} images:\")\n",
    "        for img in images[:10]:  # 只打印前10个，避免输出过多\n",
    "            print(img)\n",
    "        if len(images) > 10:\n",
    "            print(f\"... and {len(images) - 10} more\")\n",
    "\n",
    "    # 打印汇总结果\n",
    "    print(\"\\n汇总结果：\")\n",
    "    for cls, images in all_class_images.items():\n",
    "        print(f\"  Class {cls}: {len(images)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No delimiter ':' found in line 'Class IoUs:'\n",
      "Warning: No delimiter ':' found in line 'Class IoUs:'\n",
      "Warning: No delimiter ':' found in line 'Class IoUs:'\n",
      "Warning: No delimiter ':' found in line 'Class IoUs:'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "base_dir = 'visualization_results'\n",
    "save_dir = os.path.join(base_dir, 'dataset_review')\n",
    "os.makedirs(save_dir, exist_ok=True)  # 确保保存目录存在\n",
    "\n",
    "model_dirs = [\n",
    "    'DeepLabV3_Resnet101',\n",
    "    'FCN_ResNet50',\n",
    "    'Mask2Former',\n",
    "    'Unet'\n",
    "]\n",
    "file_name = 'iou_results.txt'\n",
    "\n",
    "file_paths = [os.path.join(base_dir, model_dir, file_name) for model_dir in model_dirs]\n",
    "model_names = [\"DeepLabV3 Resnet101\", \"FCN ResNet50\", \"Mask2Former\", \"Unet\"]\n",
    "mean_ious = []\n",
    "class_iou_dict = {}\n",
    "\n",
    "# 读取文件数据\n",
    "for file_path, model_name in zip(file_paths, model_names):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        mean_iou = float(lines[0].split(': ')[1].strip())\n",
    "        mean_ious.append(mean_iou)\n",
    "        \n",
    "        for line in lines[2:]:\n",
    "            if ': ' in line:\n",
    "                parts = line.split(': ')\n",
    "                if len(parts) == 2:\n",
    "                    class_name = parts[0].split(' ')[1]\n",
    "                    iou = float(parts[1].strip())\n",
    "                    if class_name not in class_iou_dict:\n",
    "                        class_iou_dict[class_name] = []\n",
    "                    class_iou_dict[class_name].append(iou)\n",
    "                else:\n",
    "                    print(f\"Warning: Unexpected format in line '{line.strip()}'\")\n",
    "            else:\n",
    "                print(f\"Warning: No delimiter ':' found in line '{line.strip()}'\")\n",
    "\n",
    "# 创建 Mean IoU DataFrame\n",
    "mean_iou_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Mean IoU': mean_ious\n",
    "})\n",
    "\n",
    "# 创建 Class IoU DataFrame\n",
    "iou_data = {class_name: [class_iou_dict[class_name][i] if i < len(class_iou_dict[class_name]) else None for i in range(len(model_names))] for class_name in class_iou_dict}\n",
    "class_iou_df = pd.DataFrame(iou_data, index=model_names)\n",
    "\n",
    "# 生成 LaTeX 表格\n",
    "mean_iou_latex = mean_iou_df.to_latex(float_format=\"%.2f\", caption=\"Mean IoU Comparison Table\", label=\"tab:mean_iou_comparison\")\n",
    "with open(os.path.join(save_dir, 'mean_iou_comparison_table.tex'), 'w') as f:\n",
    "    f.write(mean_iou_latex)\n",
    "\n",
    "class_iou_latex = class_iou_df.to_latex(float_format=\"%.2f\", caption=\"Class IoU Comparison Table\", label=\"tab:class_iou_comparison\")\n",
    "with open(os.path.join(save_dir, 'class_iou_comparison_table.tex'), 'w') as f:\n",
    "    f.write(class_iou_latex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No delimiter ':' found in line 'Class IoUs:'\n",
      "Warning: No delimiter ':' found in line 'Class IoUs:'\n",
      "Warning: No delimiter ':' found in line 'Class IoUs:'\n",
      "Warning: No delimiter ':' found in line 'Class IoUs:'\n",
      "                          0       1       2       3    4       5       6  \\\n",
      "DeepLabV3 Resnet101  0.7240  0.1602  0.6591  0.3196  0.0  0.3819  0.8131   \n",
      "FCN ResNet50         0.7562  0.2913  0.6996  0.4130  0.0  0.3743  0.8169   \n",
      "Mask2Former          0.6842  0.2164  0.5367  0.4369  0.0  0.3969  0.8298   \n",
      "Unet                 0.7226  0.2163  0.6434  0.2503  0.0  0.5368  0.8481   \n",
      "\n",
      "                          7       8       9   10      11      12      13  \\\n",
      "DeepLabV3 Resnet101  0.2737  0.0966  0.1611  0.0  0.3311  0.1825  0.1263   \n",
      "FCN ResNet50         0.3637  0.3272  0.1900  0.0  0.4078  0.2388  0.1505   \n",
      "Mask2Former          0.1235  0.0000  0.1498  0.0  0.3179  0.1073  0.1224   \n",
      "Unet                 0.3122  0.0000  0.1170  0.0  0.3964  0.2512  0.2471   \n",
      "\n",
      "                         14   15      16  \n",
      "DeepLabV3 Resnet101  0.6510  0.0  0.4816  \n",
      "FCN ResNet50         0.6916  0.0  0.4909  \n",
      "Mask2Former          0.6335  0.0  0.6018  \n",
      "Unet                 0.6921  0.0  0.5461  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "base_dir = 'visualization_results'\n",
    "save_dir = os.path.join(base_dir, 'dataset_review')\n",
    "os.makedirs(save_dir, exist_ok=True)  # 确保保存目录存在\n",
    "\n",
    "model_dirs = [\n",
    "    'DeepLabV3_Resnet101',\n",
    "    'FCN_ResNet50',\n",
    "    'Mask2Former',\n",
    "    'Unet'\n",
    "]\n",
    "file_name = 'iou_results.txt'\n",
    "\n",
    "file_paths = [os.path.join(base_dir, model_dir, file_name) for model_dir in model_dirs]\n",
    "model_names = [\"DeepLabV3 Resnet101\", \"FCN ResNet50\", \"Mask2Former\", \"Unet\"]\n",
    "class_names = []\n",
    "iou_data = []\n",
    "\n",
    "# 读取文件数据\n",
    "for file_path, model_name in zip(file_paths, model_names):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        # 跳过第一行的 Mean IoU\n",
    "        model_iou = []\n",
    "        for line in lines[2:]:\n",
    "            if ': ' in line:\n",
    "                parts = line.split(': ')\n",
    "                if len(parts) == 2:\n",
    "                    class_name = parts[0].split(' ')[1]\n",
    "                    iou = float(parts[1].strip())\n",
    "                    if class_name not in class_names:\n",
    "                        class_names.append(class_name)\n",
    "                    model_iou.append(iou)\n",
    "                else:\n",
    "                    print(f\"Warning: Unexpected format in line '{line.strip()}'\")\n",
    "            else:\n",
    "                print(f\"Warning: No delimiter ':' found in line '{line.strip()}'\")\n",
    "        iou_data.append(model_iou)\n",
    "\n",
    "# 创建 DataFrame\n",
    "iou_df = pd.DataFrame(iou_data, index=model_names, columns=class_names)\n",
    "\n",
    "# 打印表格\n",
    "print(iou_df)\n",
    "\n",
    "# 生成LaTeX表格\n",
    "latex_table = iou_df.to_latex(float_format=\"%.2f\", caption=\"IoU Comparison Table\", label=\"tab:iou_comparison\")\n",
    "with open(os.path.join(save_dir, 'iou_comparison_table.tex'), 'w') as f:\n",
    "    f.write(latex_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
